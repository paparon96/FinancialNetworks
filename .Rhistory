# Cluster4 40.7 59.3 177
# Cluster5 59.6 40.4  47
# Cluster6 38.9 61.1 234
round(100*apply(cluster.sex, 2, sum) / sum(cluster.sex),2)
#     F     M
# 45.04 54.96
par(mar=c(4,4,1,1), mfrow=c(1,1))
plot(0, 0, type="n", xlab="", ylab="", bty="n", xaxt="n", yaxt="n", xlim=c(-30,100), ylim=c(0,7))
for(i in 1:6) {
rect(0,7-i-0.4,cluster.sex.perc[i,1],7-i+0.4, col=col.clus[i], border=col.clus[i], lwd=2)
rect(cluster.sex.perc[i,1]+1.5,7-i-0.4, 100, 7-i+0.4, col="white", border=col.clus[i], lwd=2)
text(-5, 7-i, labels=paste("Cluster",i), font=4, col=col.clus[i], pos=2)
}
axis(1, at=seq(0, 100, 20))
summary(aov(age ~ factor(vars.km6$cluster)))
#                           Df Sum Sq Mean Sq F value   Pr(>F)
# factor(vars.km6$cluster)   5   3654   730.7   5.575 4.54e-05 ***
# Residuals                922 120840   131.1
## univariate confidence plots
require(boot)
CIplot_uni(age, group=factor(vars.km6$cluster), ylim=c(35,52), cols=col.clus, shade=T, alpha=1, thin=2.5)
## global improvement
global <- c(prog[,1], prog[,4], prog[,7])
months <- c(rep(12, 928), rep(3, 928), rep(0.5, 928))
gpsrep <- rep(vars.km6$cluster, 3)
tab <- aggregate(global ~ factor(months) + factor(gpsrep), FUN=mean)
adjust <- c(0,-0.02,0.02,0,-0.02,0.05)
plot(0, 0, type="n", xlab="", ylab="", bty="n", xaxt="n", yaxt="n", xlim=c(0,17), ylim=c(1.4,3.0))
for(i in 1:6) {
lines(c(0.5,3,12), tab[(3*i-2):(3*i),"global"], col=col.clus[i], lwd=3)
points(c(0.5,3,12), tab[(3*i-2):(3*i),"global"], col=col.clus[i], bg="white", pch=21, cex=1.2, font=2)
text(12, tab[3*i, "global"]+adjust[i], pos=4, labels=paste("Cluster", i), font=4, col=col.clus[i])
}
axis(1, at=c(0.5,3,12), labels=c("0.5","3","12"))
axis(2, las=2, at=c(1.5,2.0,2.5,3.0))
# LBP intensity
LBPint <- c(prog[,2], prog[,5], prog[,8])
months <- c(rep(12, 928), rep(3, 928), rep(0.5, 928))
gpsrep <- rep(vars.km6$cluster, 3)
tab2 <- aggregate(LBPint ~ factor(months) + factor(gpsrep), FUN=mean)
adjust <- c(0,0.05,0.1,-0.05,0.05,-0.15)
plot(0, 0, type="n", xlab="", ylab="", bty="n", xaxt="n", yaxt="n", xlim=c(0,17), ylim=c(0.8,5))
for(i in 1:6) {
lines(c(0.5,3,12), tab2[(3*i-2):(3*i),"LBPint"], col=col.clus[i], lwd=3)
points(c(0.5,3,12), tab2[(3*i-2):(3*i),"LBPint"], col=col.clus[i], bg="white", pch=21, cex=1.2, font=2)
text(12, tab2[3*i, "LBPint"]+adjust[i], pos=4, labels=paste("Cluster", i), font=4, col=col.clus[i])
}
axis(1, at=c(0.5,3,12), labels=c("0.5","3","12"))
axis(2, las=2)
# Roland-Morris
RoMo   <- c(prog[,3], prog[,6], prog[,9])
months <- c(rep(12, 928), rep(3, 928), rep(0.5, 928))
gpsrep <- rep(vars.km6$cluster, 3)
tab3 <- aggregate(RoMo ~ factor(months) + factor(gpsrep), FUN=mean)
adjust <- c(0,0.5,-0.5,1.5,0,-2)
plot(0, 0, type="n", xlab="", ylab="", bty="n", xaxt="n", yaxt="n", xlim=c(0,17), ylim=c(0,60))
for(i in 1:6) {
lines(c(0.5,3,12), tab3[(3*i-2):(3*i),"RoMo"], col=col.clus[i], lwd=3)
points(c(0.5,3,12), tab3[(3*i-2):(3*i),"RoMo"], col=col.clus[i], bg="white", pch=21, cex=1.2, font=2)
text(12, tab3[3*i, "RoMo"]+adjust[i], pos=4, labels=paste("Cluster", i), font=4, col=col.clus[i])
}
axis(1, at=c(0.5,3,12), labels=c("0.5","3","12"))
axis(2, las=2)
# Import packages
library(circlize)
# Import data
temp = read.csv("./Data/Estimated_networks/Return_spillover.csv", sep=",", row.names=1)
# Transform to matrix format
network_matrix = as.matrix(temp)
setwd("/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks")
# Import packages
library(circlize)
# Import data
temp = read.csv("./Data/Estimated_networks/SPACE.csv", sep=",", row.names=1)
# Transform to matrix format
network_matrix = as.matrix(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Set working directory
setwd("/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks")
# Import packages
library(circlize)
# Import data
temp = read.csv("./Data/Estimated_networks/Return_spillover.csv", sep=",", row.names=1)
View(temp)
temp = temp[1:10,1:10]
View(temp)
# Transform to matrix format
network_matrix = as.matrix(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Create vectors for network visualisation
n = length(varnames)*(length(varnames)-1)
orig = vector(mode="character", length=n)
dest = vector(mode="character", length=n)
weight <- rep(0, n)
# Fill vectors with the data
k = 0
for (i in 1:length(varnames)){
for (j in 1:length(varnames)){
if (i != j){
orig[k] = varnames[i]
dest[k] = varnames[j]
weight[k] = network_matrix[i,j]
k = k + 1
}
}
}
# Create dataframe from vectors
df<- data.frame(orig, dest,
weight)
# Visualisation
rownames(network_matrix) = varnames
colnames(network_matrix) = varnames
diag(network_matrix) <- 0
circos.clear()
title = "./Figures/DY_return_method_network_chord_diagram.pdf"
pdf(title)
chordDiagram(network_matrix, directional = 1,
direction.type = c("diffHeight", "arrows"),
link.arr.type = "big.arrow")
dev.off()
## OPTION 2: return - with factor
# Import data
temp = read.csv("./Data/Estimated_networks/Return_spillover_resid.csv", sep=",", row.names=1)
# Get relevant entries
temp = temp[1:10,1:10]
# Transform to matrix format
network_matrix = as.matrix(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Create vectors for network visualisation
n = length(varnames)*(length(varnames)-1)
orig = vector(mode="character", length=n)
dest = vector(mode="character", length=n)
weight <- rep(0, n)
# Fill vectors with the data
k = 0
for (i in 1:length(varnames)){
for (j in 1:length(varnames)){
if (i != j){
orig[k] = varnames[i]
dest[k] = varnames[j]
weight[k] = network_matrix[i,j]
k = k + 1
}
}
}
# Create dataframe from vectors
df<- data.frame(orig, dest,
weight)
# Visualisation
rownames(network_matrix) = varnames
colnames(network_matrix) = varnames
diag(network_matrix) <- 0
circos.clear()
title = "./Figures/DY_return_resid_method_network_chord_diagram.pdf"
pdf(title)
chordDiagram(network_matrix, directional = 1,
direction.type = c("diffHeight", "arrows"),
link.arr.type = "big.arrow")
dev.off()
View(temp)
temp = temp / 10
View(temp)
# Import data
temp = read.csv("./Data/Estimated_networks/Return_spillover.csv", sep=",", row.names=1)
# Get relevant entries
temp = temp[1:10,1:10]
temp = temp / 100
View(temp)
# Transform to matrix format
network_matrix = as.matrix(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Create vectors for network visualisation
n = length(varnames)*(length(varnames)-1)
orig = vector(mode="character", length=n)
dest = vector(mode="character", length=n)
weight <- rep(0, n)
# Fill vectors with the data
k = 0
for (i in 1:length(varnames)){
for (j in 1:length(varnames)){
if (i != j){
orig[k] = varnames[i]
dest[k] = varnames[j]
weight[k] = network_matrix[i,j]
k = k + 1
}
}
}
# Create dataframe from vectors
df<- data.frame(orig, dest,
weight)
# Visualisation
rownames(network_matrix) = varnames
colnames(network_matrix) = varnames
diag(network_matrix) <- 0
circos.clear()
title = "./Figures/DY_return_method_network_chord_diagram.pdf"
pdf(title)
chordDiagram(network_matrix, directional = 1,
direction.type = c("diffHeight", "arrows"),
link.arr.type = "big.arrow")
dev.off()
View(temp)
## OPTION 2: return - with factor
# Import data
temp = read.csv("./Data/Estimated_networks/Return_spillover_resid.csv", sep=",", row.names=1)
# Get relevant entries
temp = temp[1:10,1:10]
temp = temp / 100
# Transform to matrix format
network_matrix = as.matrix(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Create vectors for network visualisation
n = length(varnames)*(length(varnames)-1)
orig = vector(mode="character", length=n)
dest = vector(mode="character", length=n)
weight <- rep(0, n)
# Fill vectors with the data
k = 0
for (i in 1:length(varnames)){
for (j in 1:length(varnames)){
if (i != j){
orig[k] = varnames[i]
dest[k] = varnames[j]
weight[k] = network_matrix[i,j]
k = k + 1
}
}
}
# Create dataframe from vectors
df<- data.frame(orig, dest,
weight)
# Visualisation
rownames(network_matrix) = varnames
colnames(network_matrix) = varnames
diag(network_matrix) <- 0
circos.clear()
title = "./Figures/DY_return_resid_method_network_chord_diagram.pdf"
pdf(title)
chordDiagram(network_matrix, directional = 1,
direction.type = c("diffHeight", "arrows"),
link.arr.type = "big.arrow")
dev.off()
## OPTION 3: volatility - no factor
# Import data
temp = read.csv("./Data/Estimated_networks/Volatility_spillover.csv", sep=",", row.names=1)
# Get relevant entries
temp = temp[1:10,1:10]
temp = temp / 100
View(temp)
# Transform to matrix format
network_matrix = as.matrix(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Create vectors for network visualisation
n = length(varnames)*(length(varnames)-1)
orig = vector(mode="character", length=n)
dest = vector(mode="character", length=n)
weight <- rep(0, n)
# Fill vectors with the data
k = 0
for (i in 1:length(varnames)){
for (j in 1:length(varnames)){
if (i != j){
orig[k] = varnames[i]
dest[k] = varnames[j]
weight[k] = network_matrix[i,j]
k = k + 1
}
}
}
# Create dataframe from vectors
df<- data.frame(orig, dest,
weight)
# Visualisation
rownames(network_matrix) = varnames
colnames(network_matrix) = varnames
diag(network_matrix) <- 0
circos.clear()
title = "./Figures/DY_volatility_method_network_chord_diagram.pdf"
pdf(title)
chordDiagram(network_matrix, directional = 1,
direction.type = c("diffHeight", "arrows"),
link.arr.type = "big.arrow")
dev.off()
## OPTION 4: volatility - with factor
# Import data
temp = read.csv("./Data/Estimated_networks/Volatility_spillover_resid.csv", sep=",", row.names=1)
# Get relevant entries
temp = temp[1:10,1:10]
temp = temp / 100
# Transform to matrix format
network_matrix = as.matrix(temp)
View(temp)
# Parameters
varnames = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
# Create vectors for network visualisation
n = length(varnames)*(length(varnames)-1)
orig = vector(mode="character", length=n)
dest = vector(mode="character", length=n)
weight <- rep(0, n)
# Fill vectors with the data
k = 0
for (i in 1:length(varnames)){
for (j in 1:length(varnames)){
if (i != j){
orig[k] = varnames[i]
dest[k] = varnames[j]
weight[k] = network_matrix[i,j]
k = k + 1
}
}
}
# Create dataframe from vectors
df<- data.frame(orig, dest,
weight)
# Visualisation
rownames(network_matrix) = varnames
colnames(network_matrix) = varnames
diag(network_matrix) <- 0
circos.clear()
title = "./Figures/DY_volatility_resid_method_network_chord_diagram.pdf"
pdf(title)
chordDiagram(network_matrix, directional = 1,
direction.type = c("diffHeight", "arrows"),
link.arr.type = "big.arrow")
dev.off()
View(temp)
# Import packages
library(space)
library(glasso)
library(dplyr)
library(ggplot2)
# Import custom functions
source("common_functions.R")
# Import data
df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
## Global parameters
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
N = dim(stock_df)[1]
window_length = 150
final_date = as.Date("2020-06-30")
final_date_loc = match(final_date,as.Date(df$Date))
start_date_loc = final_date_loc - window_length #final_date - window_length
# Filter dataset based on the dates
#filtered_df = subset(df,as.Date(Date) >= start_date)
#filtered_df = subset(filtered_df, final_date > as.Date(Date))
filtered_df = df[start_date_loc:(final_date_loc-1),]
filtered_df = filtered_df[var_cols]
# Inspect the results
plot(filtered_df$MS)
# Standardize final dataset
filtered_df <- scale(filtered_df)
## Run the model
# Neighbourhood selection
#################### estimate the partial correlation matrix with various methods
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
n=nrow(filtered_df)
p=ncol(filtered_df)
result1=space.neighbor(data.matrix(filtered_df), lam1=l1, lam2=0)
print(result1)
estimated_partial_corr_matrix = result1$ParCor
# Hyperparameter tuning - penalty terms
## Export results
write.csv(estimated_partial_corr_matrix,'./Data/Estimated_networks/NE.csv')
## METHOD PART 2: Run combined model
# Import factors
five_factors_df <- read.table('./Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV',sep=",", header=TRUE)
## parameters
var_cols = c("Mkt.RF","SMB","HML","RMW","CMA","RF")
window_length = 150
final_date = "30/06/2020"
final_date_loc = match(final_date,five_factors_df$Date)
start_date_loc = final_date_loc - window_length #final_date - window_length
filtered_factor_df = five_factors_df[start_date_loc:(final_date_loc-1),]
filtered_factor_df = filtered_factor_df[var_cols]
# Inspect the results
plot(filtered_factor_df$SMB)
# Standardize final dataset
filtered_factor_df <- scale(filtered_factor_df)
# Combine arrays
combined_filtered_df <- cbind(filtered_df,filtered_factor_df)
final_date_loc
start_date_loc
# Import factors
five_factors_df <- read.table('./Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV',sep=",", header=TRUE)
## parameters
var_cols = c("Mkt.RF","SMB","HML","RMW","CMA","RF")
window_length = 150
final_date = "30/06/2020"
final_date_loc = match(final_date,five_factors_df$Date)
start_date_loc = final_date_loc - window_length #final_date - window_length
filtered_factor_df = five_factors_df[start_date_loc:(final_date_loc-1),]
filtered_factor_df = filtered_factor_df[var_cols]
# Inspect the results
plot(filtered_factor_df$SMB)
# Standardize final dataset
filtered_factor_df <- scale(filtered_factor_df)
# Combine arrays
combined_filtered_df <- cbind(filtered_df,filtered_factor_df)
# Import packages
library(space)
library(glasso)
library(dplyr)
library(ggplot2)
# Import custom functions
source("common_functions.R")
# Import data
df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
## Global parameters
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
N = dim(stock_df)[1]
window_length = 150
final_date = as.Date("2020-06-30")
final_date_loc = match(final_date,as.Date(df$Date))
start_date_loc = final_date_loc - window_length #final_date - window_length
# Filter dataset based on the dates
#filtered_df = subset(df,as.Date(Date) >= start_date)
#filtered_df = subset(filtered_df, final_date > as.Date(Date))
filtered_df = df[start_date_loc:(final_date_loc-1),]
filtered_df = filtered_df[var_cols]
# Inspect the results
plot(filtered_df$MS)
# Standardize final dataset
filtered_df <- scale(filtered_df)
# Import data
df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
## Global parameters
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
#N = dim(stock_df)[1]
window_length = 150
final_date = as.Date("2020-06-30")
final_date_loc = match(final_date,as.Date(df$Date))
start_date_loc = final_date_loc - window_length #final_date - window_length
# Filter dataset based on the dates
#filtered_df = subset(df,as.Date(Date) >= start_date)
#filtered_df = subset(filtered_df, final_date > as.Date(Date))
filtered_df = df[start_date_loc:(final_date_loc-1),]
filtered_df = filtered_df[var_cols]
final_date_loc
View(df)
View(df)
# Import custom functions
source("common_functions.R")
# Import data
df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
## Global parameters
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
#N = dim(stock_df)[1]
window_length = 150
final_date = "30/06/2020"   #as.Date("2020-06-30")
final_date_loc = match(final_date,as.Date(df$Date))
final_date_loc
View(temp)
View(df)
final_date_loc = match(final_date,df$Date)
start_date_loc = final_date_loc - window_length #final_date - window_length
# Filter dataset based on the dates
#filtered_df = subset(df,as.Date(Date) >= start_date)
#filtered_df = subset(filtered_df, final_date > as.Date(Date))
filtered_df = df[start_date_loc:(final_date_loc-1),]
filtered_df = filtered_df[var_cols]
# Inspect the results
plot(filtered_df$MS)
# Standardize final dataset
filtered_df <- scale(filtered_df)
combined_filtered_df <- cbind(filtered_df,filtered_factor_df)
## Run the model
# Neighbourhood selection
#################### estimate the partial correlation matrix with various methods
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
n=nrow(combined_filtered_df)
p=ncol(combined_filtered_df)
result1=space.neighbor(data.matrix(combined_filtered_df), lam1=l1, lam2=0)
print(result1)
estimated_partial_corr_matrix = result1$ParCor
# Hyperparameter tuning - penalty terms
## Export results
write.csv(estimated_partial_corr_matrix,'./Data/Estimated_networks/NE_combined_factors.csv')
n=nrow(combined_filtered_df)
p=ncol(combined_filtered_df)
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
result1=space.neighbor(data.matrix(combined_filtered_df), lam1=l1, lam2=0)
print(result1)
estimated_partial_corr_matrix = result1$ParCor
# Hyperparameter tuning - penalty terms
## Export results
write.csv(estimated_partial_corr_matrix,'./Data/Estimated_networks/NE_combined_factors.csv')
factor_residual_matrix <- get_residuals(filtered_df,filtered_factor_df)
factor_residual_matrix <- scale(factor_residual_matrix)
# Neighbourhood selection
#################### estimate the partial correlation matrix with various methods
n=nrow(factor_residual_matrix)
p=ncol(factor_residual_matrix)
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
result1=space.neighbor(data.matrix(factor_residual_matrix), lam1=l1, lam2=0)
print(result1)
estimated_partial_corr_matrix = result1$ParCor
write.csv(estimated_partial_corr_matrix,'./Data/Estimated_networks/NE_factor_residuals.csv')
