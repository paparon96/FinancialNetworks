filename = paste0("./Data/Estimated_networks/",method,"_",type,"_combined_factors",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
## METHOD PART 3: Model with residuals from factor regression
factor_residual_matrix <- get_residuals(filtered_df,filtered_factor_df)
factor_residual_matrix <- scale(factor_residual_matrix)
## Run the model
# Neighbourhood selection
#################### estimate the partial correlation matrix with various methods
n=nrow(factor_residual_matrix)
p=ncol(factor_residual_matrix)
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
result1=space.neighbor(data.matrix(factor_residual_matrix), lam1=l1, lam2=0)
print(result1)
estimated_partial_corr_matrix = result1$ParCor
# Hyperparameter tuning - penalty terms
## Export results
#write.csv(estimated_partial_corr_matrix,'./Data/Estimated_networks/NE_factor_residuals.csv')
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_factor_resid_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NE_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NS_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NS_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NS_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NS_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NS_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/NS_method.R')
# Import packages
library(space)
library(glasso)
library(dplyr)
library(ggplot2)
# Import custom functions
source("common_functions.R")
## Global parameters
method = "NS"
type = "volatility"
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
window_length = 150
final_date = as.Date("2020-06-30")
###### Import data
# Returns
return_df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
# Format date column
return_df$Date <- as.Date( format( as.Date( return_df$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
# Volatilities #############
vol <- list()
vol$data <- list()
vol$data$high_low <- cbind( as.data.frame( read.csv( "Data/Stock_prices/high_price.csv", header = TRUE, sep = "," ) ),
as.data.frame( read.csv( "Data/Stock_prices/low_price.csv", header = TRUE, sep = "," ) )[ , -1 ] )
colnames( vol$data$high_low )[ 1 ] <- "Date"
vol$data$high_low$Date <- as.Date( format( as.Date( vol$data$high_low$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
risk_factors_5 <- as.data.frame( read.csv( "Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV", header = TRUE, sep = "," ))
# Calculating the annualized daily percent standard deviation
vol$data$volatility <- as.data.frame( matrix( nrow = nrow( vol$data$high_low ), ncol = ncol( vol$data$high_low[ , -1 ] ) / 2 + 1 ) )
vol$data$volatility[ , 1 ] <- vol$data$high_low$Date
for( i in 1:10 ){
vol$data$volatility[ , i + 1 ] <- log( 100 * sqrt( 365 * 0.361 * ( log( vol$data$high_low[ , i + 1 ] ) -  log( vol$data$high_low[ , i + 11 ] ) )^2 ) )
}
colnames( vol$data$volatility ) <- colnames( vol$data$high_low )[ 1:11 ]
volatility_df = vol$data$volatility
#################
# Get modelling dataframe based on the current modelling type
if (type=="return"){
df = return_df
} else if (type=="volatility"){
df = volatility_df
} else{
print("Invalid modelling type defined, choose from 'return' or 'volatility'!")
}
# Filter dataset based on the dates
filtered_df = subset(df, final_date >= as.Date(Date))
start_date = tail(filtered_df$Date,n=window_length)[1]
filtered_df = subset(filtered_df,as.Date(Date) >= start_date)
# Inspect the results
plot(filtered_df$MS)
# Get only the relevant columns
filtered_df = filtered_df[var_cols]
# Standardize final dataset
filtered_df <- scale(filtered_df)
## Run the model
method = "SPACE"
# SPACE method
#################### estimate the partial correlation matrix with various methods
# Parameters
n=nrow(filtered_df)
p=ncol(filtered_df)
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
result2=space.joint(data.matrix(filtered_df), lam1=l1*n*1.56, lam2=0, iter=iter)
print(result2)
estimated_partial_corr_matrix = result2$ParCor
# Hyperparameter tuning - penalty terms
## Export results
# Name the file properly first
final_date_transformed = paste0(substr(final_date,1,4),"_",
substr(final_date,6,7),"_",
substr(final_date,9,10))
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
## METHOD PART 2: Run combined model
# Import factors
five_factors_df <- read.table('./Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV',sep=",", header=TRUE)
# Format date column
five_factors_df$Date <- as.Date( format( as.Date( five_factors_df$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
## parameters
factor_var_cols = c("Mkt.RF","SMB","HML","RMW","CMA","RF")
# Filter dataset based on the dates
filtered_factor_df = subset(five_factors_df, final_date >= as.Date(Date))
#start_date = tail(filtered_factor_df$Date,n=window_length)[1]
filtered_factor_df = subset(filtered_factor_df,as.Date(Date) >= start_date)
# Get only the relevant columns
filtered_factor_df = filtered_factor_df[factor_var_cols]
# Inspect the results
plot(filtered_factor_df$SMB)
# Standardize final dataset
filtered_factor_df <- scale(filtered_factor_df)
# Combine arrays
combined_filtered_df <- cbind(filtered_df,filtered_factor_df)
## Run the model
# Parameters
n=nrow(combined_filtered_df)
p=ncol(combined_filtered_df)
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
result2=space.joint(data.matrix(combined_filtered_df), lam1=l1*n*1.56, lam2=0, iter=iter)
print(result2)
estimated_partial_corr_matrix = result2$ParCor
## Export results
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_combined_factors_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
## METHOD PART 3: Model with residuals from factor regression
factor_residual_matrix <- get_residuals(filtered_df,filtered_factor_df)
factor_residual_matrix <- scale(factor_residual_matrix)
## Run the model
# Parameters
n=nrow(factor_residual_matrix)
p=ncol(factor_residual_matrix)
alpha=1
l1=(1/sqrt(n)*qnorm(1-alpha/(2*p^2)))*0.7
iter=3
result2=space.joint(data.matrix(factor_residual_matrix), lam1=l1*n*1.56, lam2=0, iter=iter)
print(result2)
estimated_partial_corr_matrix = result2$ParCor
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_factor_resid_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/SPACE_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/SPACE_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/SPACE_method.R')
# Import packages
library(space)
library(glasso)
library(dplyr)
library(ggplot2)
library(corpcor)
# Import custom functions
source("common_functions.R")
## Global parameters
method = "GLASSO"
type = "volatility"
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
window_length = 150
final_date = as.Date("2020-03-05")
###### Import data
# Returns
return_df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
# Format date column
return_df$Date <- as.Date( format( as.Date( return_df$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
# Volatilities #############
vol <- list()
vol$data <- list()
vol$data$high_low <- cbind( as.data.frame( read.csv( "Data/Stock_prices/high_price.csv", header = TRUE, sep = "," ) ),
as.data.frame( read.csv( "Data/Stock_prices/low_price.csv", header = TRUE, sep = "," ) )[ , -1 ] )
colnames( vol$data$high_low )[ 1 ] <- "Date"
vol$data$high_low$Date <- as.Date( format( as.Date( vol$data$high_low$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
risk_factors_5 <- as.data.frame( read.csv( "Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV", header = TRUE, sep = "," ))
# Calculating the annualized daily percent standard deviation
vol$data$volatility <- as.data.frame( matrix( nrow = nrow( vol$data$high_low ), ncol = ncol( vol$data$high_low[ , -1 ] ) / 2 + 1 ) )
vol$data$volatility[ , 1 ] <- vol$data$high_low$Date
for( i in 1:10 ){
vol$data$volatility[ , i + 1 ] <- log( 100 * sqrt( 365 * 0.361 * ( log( vol$data$high_low[ , i + 1 ] ) -  log( vol$data$high_low[ , i + 11 ] ) )^2 ) )
}
colnames( vol$data$volatility ) <- colnames( vol$data$high_low )[ 1:11 ]
volatility_df = vol$data$volatility
#################
# Get modelling dataframe based on the current modelling type
if (type=="return"){
df = return_df
} else if (type=="volatility"){
df = volatility_df
} else{
print("Invalid modelling type defined, choose from 'return' or 'volatility'!")
}
# Filter dataset based on the dates
filtered_df = subset(df, final_date >= as.Date(Date))
start_date = tail(filtered_df$Date,n=window_length)[1]
filtered_df = subset(filtered_df,as.Date(Date) >= start_date)
# Inspect the results
plot(filtered_df$MS)
# Get only the relevant columns
filtered_df = filtered_df[var_cols]
# Standardize final dataset
filtered_df <- scale(filtered_df)
## METHOD 1: Base case (without factors)
## Run the model
# GLASSO method
#################### estimate the partial correlation matrix with various methods
# Parameters
# Get the sample covariance matrix of the data
s <- var(filtered_df)
rho_param <- .01
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated covariance matrix
estim_covmat = result3$w
## Transform the covariance matrix into a partial correlation matrix
estimated_partial_corr_matrix = cor2pcor(estim_covmat)
print(estimated_partial_corr_matrix)
final_date_transformed = paste0(substr(final_date,1,4),"_",
substr(final_date,6,7),"_",
substr(final_date,9,10))
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
# Import factors
five_factors_df <- read.table('./Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV',sep=",", header=TRUE)
# Format date column
five_factors_df$Date <- as.Date( format( as.Date( five_factors_df$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
## parameters
factor_var_cols = c("Mkt.RF","SMB","HML","RMW","CMA","RF")
# Filter dataset based on the dates
filtered_factor_df = subset(five_factors_df, final_date >= as.Date(Date))
#start_date = tail(filtered_factor_df$Date,n=window_length)[1]
filtered_factor_df = subset(filtered_factor_df,as.Date(Date) >= start_date)
# Get only the relevant columns
filtered_factor_df = filtered_factor_df[factor_var_cols]
# Inspect the results
plot(filtered_factor_df$SMB)
# Standardize final dataset
filtered_factor_df <- scale(filtered_factor_df)
# Combine arrays
combined_filtered_df <- cbind(filtered_df,filtered_factor_df)
# Get the sample covariance matrix of the data
s <- var(combined_filtered_df)
rho_param <- 0.85 #0.01 #0.85
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated inverse covariance matrix
estim_inverse_partial_covmat = result3$w
# Get the partial correlation network from the inverse covariance matrix
estimated_partial_corr_matrix = cor2pcor(estim_inverse_partial_covmat)
print(estimated_partial_corr_matrix)
# Get the sample covariance matrix of the data
s <- var(combined_filtered_df)
rho_param <- 0.35 #0.01 #0.85
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated inverse covariance matrix
estim_inverse_partial_covmat = result3$w
# Get the partial correlation network from the inverse covariance matrix
estimated_partial_corr_matrix = cor2pcor(estim_inverse_partial_covmat)
print(estimated_partial_corr_matrix)
rho_param <- 0.55 #0.01 #0.85
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated inverse covariance matrix
estim_inverse_partial_covmat = result3$w
# Get the partial correlation network from the inverse covariance matrix
estimated_partial_corr_matrix = cor2pcor(estim_inverse_partial_covmat)
print(estimated_partial_corr_matrix)
rho_param <- 0.75 #0.01 #0.85
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated inverse covariance matrix
estim_inverse_partial_covmat = result3$w
# Get the partial correlation network from the inverse covariance matrix
estimated_partial_corr_matrix = cor2pcor(estim_inverse_partial_covmat)
print(estimated_partial_corr_matrix)
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_combined_factors_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
rho_hyperparam = 0.8
actor_residual_matrix <- get_residuals(filtered_df,filtered_factor_df)
factor_residual_matrix <- scale(factor_residual_matrix)
# Get the sample covariance matrix of the data
s <- var(factor_residual_matrix)
rho_param <- rho_hyperparam
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated inverse covariance matrix
estim_inverse_partial_covmat = result3$wi
# Get the partial correlation network from the inverse covariance matrix
estimated_partial_corr_matrix = invcov2pcorr(estim_inverse_partial_covmat)
print(estimated_partial_corr_matrix)
factor_residual_matrix <- get_residuals(filtered_df,filtered_factor_df)
factor_residual_matrix <- scale(factor_residual_matrix)
# Get the sample covariance matrix of the data
s <- var(factor_residual_matrix)
rho_param <- rho_hyperparam
# Run the algorithm
result3 <-glasso(s, rho=rho_param)
# Obtain the estimated inverse covariance matrix
estim_inverse_partial_covmat = result3$wi
# Get the partial correlation network from the inverse covariance matrix
estimated_partial_corr_matrix = invcov2pcorr(estim_inverse_partial_covmat)
print(estimated_partial_corr_matrix)
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_factor_resid_",final_date_transformed,".csv")
write.csv(estimated_partial_corr_matrix,filename)
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/GLASSO_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/GLASSO_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/GLASSO_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/GLASSO_method.R')
# Import packages
library(space)
library(glasso)
library(dplyr)
library(ggplot2)
library(corpcor)
library(sparsebn)
# Import custom functions
source("common_functions.R")
## Global parameters
method = "SPACE"
type = "volatility"
var_cols = c("MS","JPM","BAC","C","WFC","GS","USB","TD","BK","TFC")
window_length = 150
final_date = as.Date("2020-03-05")
###### Import data
# Returns
return_df <- read.table('./Data/Stock_prices/log_returns_all_ts.csv',sep=",", header=TRUE)
# Format date column
return_df$Date <- as.Date( format( as.Date( return_df$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
# Volatilities #############
vol <- list()
vol$data <- list()
vol$data$high_low <- cbind( as.data.frame( read.csv( "Data/Stock_prices/high_price.csv", header = TRUE, sep = "," ) ),
as.data.frame( read.csv( "Data/Stock_prices/low_price.csv", header = TRUE, sep = "," ) )[ , -1 ] )
colnames( vol$data$high_low )[ 1 ] <- "Date"
vol$data$high_low$Date <- as.Date( format( as.Date( vol$data$high_low$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
risk_factors_5 <- as.data.frame( read.csv( "Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV", header = TRUE, sep = "," ))
# Calculating the annualized daily percent standard deviation
vol$data$volatility <- as.data.frame( matrix( nrow = nrow( vol$data$high_low ), ncol = ncol( vol$data$high_low[ , -1 ] ) / 2 + 1 ) )
vol$data$volatility[ , 1 ] <- vol$data$high_low$Date
for( i in 1:10 ){
vol$data$volatility[ , i + 1 ] <- log( 100 * sqrt( 365 * 0.361 * ( log( vol$data$high_low[ , i + 1 ] ) -  log( vol$data$high_low[ , i + 11 ] ) )^2 ) )
}
colnames( vol$data$volatility ) <- colnames( vol$data$high_low )[ 1:11 ]
volatility_df = vol$data$volatility
#################
# Get modelling dataframe based on the current modelling type
if (type=="return"){
df = return_df
} else if (type=="volatility"){
df = volatility_df
} else{
print("Invalid modelling type defined, choose from 'return' or 'volatility'!")
}
# Filter dataset based on the dates
filtered_df = subset(df, final_date >= as.Date(Date))
start_date = tail(filtered_df$Date,n=window_length)[1]
filtered_df = subset(filtered_df,as.Date(Date) >= start_date)
# Inspect the results
plot(filtered_df$MS)
# Get only the relevant columns
filtered_df = filtered_df[var_cols]
# Standardize final dataset
filtered_df <- scale(filtered_df)
method = "DAG"
# Parameters
dat <- sparsebnData(filtered_df, type = "c")
dags = estimate.dag(dat)
dags.fit <- estimate.parameters(dags, data = dat)
# Get adjacency matrix of one particular solution from the solution path
#A <- as.matrix(get.adjacency.matrix(data_dag[[10]]))
A = dags.fit[[10]]$coefs
A = as.data.frame(summary(A))
View(A)
A = dags.fit[[10]]$coefs
A
write.csv(A,'./Data/Estimated_networks/DAG_sparsebn.csv')
estimated_network = as.data.frame(estimated_network)
estimated_network = as.data.frame(A)
summary(A)
as.data.frame(summary(A))
A = dags.fit[[10]]$coefs
A = as.data.frame(summary(A))
A = dags.fit[[10]]$coefs
b = as(A, "dgCMatrix")
cbind.data.frame(r = b@i + 1, c = b@j + 1, x = b@x)
estimated_network = as.data.frame(A)
b = as(A, "dgCMatrix")
cbind.data.frame(r = b@i + 1, c = b@p + 1, x = b@x)
corr <- matrix(, nrow = 2, ncol = 2)
corr
corr <- matrix(0, nrow = 2, ncol = 2)
corr
length(return_df)
dim(return_df)[1]
source("common_functions.R")
d = dim(filtered_df)[2]
d
estimated_network = dag_matrix(A,d,d)
source("common_functions.R")
A = dags.fit[[10]]$coefs
estimated_network = dag_matrix(A,d,d)
A
b = as(A, "dgCMatrix")
temp = cbind.data.frame(r = b@i + 1, c = b@p + 1, x = b@x)
temp
source("common_functions.R")
d = dim(filtered_df)[2]
d
estimated_network = dag_matrix(A,d,d)
A = dags.fit[[10]]$coefs
A = as.data.frame(summary(A))
# Parameters
dat <- sparsebnData(filtered_df, type = "c")
# Get final number of columns
d = dim(filtered_df)[2]
dags = estimate.dag(dat)
dags.fit <- estimate.parameters(dags, data = dat)
# Get adjacency matrix of one particular solution from the solution path
#A <- as.matrix(get.adjacency.matrix(data_dag[[10]]))
A = dags.fit[[10]]$coefs
A = as.data.frame(summary(A))
A
summary(A)
temp = summary(A)
temp
as.matrix(summary(A))
A = dags.fit[[10]]$coefs
source("common_functions.R")
estimated_network = dag_matrix(A,d,d)
source("common_functions.R")
A = dags.fit[[10]]$coefs
d
estimated_network = dag_matrix(A,d,d)
source("common_functions.R")
estimated_network = dag_matrix(A,d,d)
source("common_functions.R")
A@p
A@p[2:]
type(A@p)
typeof(A@p)
class(A@p)
list(A@p)
list(A@p)[1]
list(A@p)[1][1]
list(A@p)[1][4:]
A
as.matrix(A)
estimated_network = as.matrix(A)
# Name the file properly first
final_date_transformed = paste0(substr(final_date,1,4),"_",
substr(final_date,6,7),"_",
substr(final_date,9,10))
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_",final_date_transformed,".csv")
write.csv(estimated_network,filename)
# Import factors
five_factors_df <- read.table('./Data/Stock_prices/F-F_Research_Data_5_Factors_2x3_daily.CSV',sep=",", header=TRUE)
# Format date column
five_factors_df$Date <- as.Date( format( as.Date( five_factors_df$Date, format = "%d/%m/%Y" ), "%Y-%m-%d"))
## parameters
factor_var_cols = c("Mkt.RF","SMB","HML","RMW","CMA","RF")
# Filter dataset based on the dates
filtered_factor_df = subset(five_factors_df, final_date >= as.Date(Date))
#start_date = tail(filtered_factor_df$Date,n=window_length)[1]
filtered_factor_df = subset(filtered_factor_df,as.Date(Date) >= start_date)
# Get only the relevant columns
filtered_factor_df = filtered_factor_df[factor_var_cols]
# Inspect the results
plot(filtered_factor_df$SMB)
# Standardize final dataset
filtered_factor_df <- scale(filtered_factor_df)
# Combine arrays
combined_filtered_df <- cbind(filtered_df,filtered_factor_df)
# Parameters
dat <- sparsebnData(combined_filtered_df, type = "c")
# Run the algorithm
dags = estimate.dag(dat)
dags.fit <- estimate.parameters(dags, data = dat)
# Get adjacency matrix of one particular solution from the solution path
A = dags.fit[[10]]$coefs
# Convert sparse matrix into regular matrix format
estimated_network = as.matrix(A)
estimated_network
## Export results
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_combined_factors_",final_date_transformed,".csv")
write.csv(estimated_network,filename)
factor_residual_matrix <- get_residuals(filtered_df,filtered_factor_df)
factor_residual_matrix <- scale(factor_residual_matrix)
# Parameters
dat <- sparsebnData(factor_residual_matrix, type = "c")
# Run the algorithm
dags = estimate.dag(dat)
dags.fit <- estimate.parameters(dags, data = dat)
# Get adjacency matrix of one particular solution from the solution path
A = dags.fit[[10]]$coefs
# Convert sparse matrix into regular matrix format
estimated_network = as.matrix(A)
estimated_network
## Export results
filename = paste0("./Data/Estimated_networks/",method,"_",type,"_factor_resid_",final_date_transformed,".csv")
write.csv(estimated_network,filename)
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/DAG_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/DAG_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/DAG_method.R')
source('/Egyetem/BGSE/Studies/Research/FinNetworks/FinancialNetworks/DAG_method.R')
grepl("2020", final_Date, fixed = TRUE)
grepl("2020", final_date, fixed = TRUE)
grepl("tizen", final_date, fixed = TRUE)
